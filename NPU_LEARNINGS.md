# NPU vs CPU Performance Analysis & Best Practices

## Executive Summary

This document captures key learnings from implementing and benchmarking NPU (Neural Processing Unit) acceleration on Snapdragon X ARM64 with ONNX Runtime QNN provider for embedding generation.

## Key Performance Findings

### NPU Performance Characteristics

**âœ… NPU Excels At:**
- **Small batch sizes (1-3 texts)**: Up to 2.33x speedup
- **Single text inference**: 1.41x speedup (76ms vs 107ms)
- **Consistent latency**: Lower variance in response times
- **Quality preservation**: Identical embedding quality to CPU

**âŒ NPU Struggles With:**
- **Large batch sizes (5+ texts)**: 5-7% slower than CPU
- **Memory transfer overhead**: Becomes bottleneck for larger workloads
- **Parallel processing**: Less efficient than CPU for batch operations

### Detailed Benchmark Results

```
Batch Size | CPU Avg | NPU Avg | Speedup | Recommendation
-----------|---------|---------|---------|---------------
1 text     | 107ms   | 76ms    | 1.41x   | âœ… Use NPU
3 texts    | 546ms   | 235ms   | 2.33x   | âœ… Use NPU  
5 texts    | 354ms   | 377ms   | 0.94x   | âŒ Use CPU
10 texts   | 689ms   | 738ms   | 0.93x   | âŒ Use CPU
```

### Performance Per Text Analysis

```
Batch Size | CPU Per-Text | NPU Per-Text | Notes
-----------|--------------|--------------|-------
1 text     | 107ms        | 76ms         | NPU optimal
3 texts    | 182ms        | 78ms         | NPU sweet spot
5 texts    | 71ms         | 75ms         | CPU begins to win
10 texts   | 69ms         | 74ms         | CPU clearly better
```

## Decision Rules Implementation

### Automatic Provider Selection Logic

```python
def select_optimal_provider(batch_size: int, npu_available: bool) -> str:
    """
    Automatically select the optimal inference provider based on batch size
    
    Rules based on empirical benchmarking:
    - NPU: 1-3 texts (up to 2.33x speedup)
    - CPU: 4+ texts (more efficient for larger batches)
    """
    if not npu_available:
        return "CPU"
    
    # Sweet spot for NPU is 1-3 texts
    if batch_size <= 3:
        return "NPU"
    else:
        return "CPU"
```

### Implementation in FastAPI Gateway

```python
# Intelligent NPU/CPU routing based on batch size
use_npu = len(texts) <= 3 and app_state.npu_available

# Generate embeddings using optimal provider
embeddings_array, timing_info = app_state.embedding_engine.encode(
    texts, 
    use_npu=use_npu
)
```

## Technical Learnings

### NPU Hardware Characteristics

**âœ… Confirmed Working:**
- **Hardware**: Snapdragon X126100 Hexagon NPU
- **Software Stack**: ONNX Runtime 1.22.2 with QNN Execution Provider
- **Model Format**: Standard ONNX models (no special conversion needed)
- **Memory**: Efficient usage with minimal overhead

**ðŸ”§ Setup Requirements:**
- QNN Execution Provider must be first in provider list
- Models load successfully with automatic fallback to CPU
- No special model quantization required (works with FP32)

### Memory Usage Patterns

```
Component               | Memory Usage | Notes
------------------------|--------------|------------------------
Base FastAPI Server     | ~100MB       | Minimal overhead
BGE Model (CPU)         | ~500MB       | Standard ONNX model
BGE Model (NPU)         | ~550MB       | Slight NPU overhead
Embedding Engine        | ~50MB        | Tokenizer + utilities
Total Runtime           | ~700-800MB   | Very reasonable for 16GB system
```

### Quality Assurance

**âœ… Embedding Quality Verification:**
- CPU vs NPU embeddings: **Identical** (cosine similarity: 0.743 both)
- No quality degradation with NPU acceleration
- Deterministic results across multiple runs
- Proper normalization maintained

## Best Practices for Production

### 1. Automatic Provider Selection
```python
# âœ… GOOD: Automatic selection based on workload
use_npu = len(texts) <= 3 and npu_available

# âŒ BAD: Leaving decision to user
use_npu = request.get("use_npu", False)  # Don't do this
```

### 2. Graceful Fallback
```python
# âœ… GOOD: Always have CPU fallback
providers = ['QNNExecutionProvider', 'CPUExecutionProvider']
session = ort.InferenceSession(model_path, providers=providers)

# âŒ BAD: NPU-only with no fallback
providers = ['QNNExecutionProvider']  # Will fail if NPU unavailable
```

### 3. Performance Monitoring
```python
# âœ… GOOD: Log performance for optimization
logger.info(f"Generated {len(embeddings)} embeddings using {provider} "
           f"in {timing['total_time']:.3f}s")

# Track metrics for continuous optimization
performance_metrics = {
    "provider": provider,
    "batch_size": len(texts),
    "latency_ms": timing['total_time'] * 1000,
    "throughput_texts_per_sec": len(texts) / timing['total_time']
}
```

### 4. Error Handling
```python
# âœ… GOOD: Handle NPU initialization failures gracefully
try:
    session_npu = ort.InferenceSession(model_path, npu_providers)
    if 'QNNExecutionProvider' not in session_npu.get_providers():
        logger.warning("QNN provider not available, using CPU only")
        session_npu = None
except Exception as e:
    logger.warning(f"NPU initialization failed: {e}")
    session_npu = None
```

## Optimization Recommendations

### For Current Implementation

1. **Batch Size Optimization**: Current 3-text threshold is optimal
2. **Model Loading**: Single model instance shared across requests is efficient
3. **Memory Management**: Current usage (800MB) leaves plenty of headroom

### For Future Scaling

1. **Multi-Model Support**: Load different models based on use case
2. **Dynamic Batching**: Accumulate requests for optimal batch sizes
3. **Caching**: Cache embeddings for frequently requested texts
4. **Async Processing**: Use async/await for better concurrency

## Configuration Guidelines

### Environment Setup
```yaml
# Optimal configuration for Snapdragon X
npu:
  enabled: true
  batch_size_threshold: 3  # Use NPU for batches <= 3
  fallback_to_cpu: true
  
models:
  embedding:
    model_path: "models/bge-small-en-v1.5"
    providers: ["QNNExecutionProvider", "CPUExecutionProvider"]
    max_batch_size: 10
    
performance:
  log_timing: true
  track_provider_usage: true
```

### Hardware Requirements
- **Minimum RAM**: 2GB for model + 1GB for NPU overhead
- **NPU Support**: Qualcomm Hexagon (QNN provider required)
- **Fallback**: Always ensure CPU provider available

## Troubleshooting Guide

### Common Issues

1. **"QNN provider not available"**
   - Solution: Verify ONNX Runtime QNN package installed
   - Fallback: System automatically uses CPU

2. **"DLL load failed" on ARM64**
   - Solution: Use ONNX Runtime binaries, not QAIRT SDK
   - Verified: Direct ONNX Runtime approach works perfectly

3. **Poor NPU performance**
   - Check: Batch size > 3? Use CPU instead
   - Verify: Provider order (QNN first, CPU second)

4. **Memory issues**
   - Monitor: Current usage ~800MB is normal
   - Alert: If usage > 2GB, investigate memory leaks

## Future Research Areas

### Performance Optimization
- **Model Quantization**: Test INT8 vs FP32 performance on NPU
- **Dynamic Batching**: Implement request queuing for optimal batch sizes
- **Multi-Threading**: Explore concurrent CPU/NPU processing

### Model Exploration
- **Specialized Models**: Test models optimized specifically for NPU
- **Multi-Modal**: Evaluate vision models on NPU
- **LLM Integration**: Benchmark chat models on NPU vs CPU

## Conclusion

The NPU provides significant benefits for small-batch embedding generation (up to 2.33x speedup) but becomes less efficient for larger batches due to memory transfer overhead. The implemented automatic provider selection ensures optimal performance without user intervention.

**Key Success Factors:**
1. âœ… Automatic provider selection based on batch size
2. âœ… Graceful fallback to CPU when NPU unavailable
3. âœ… No quality degradation with NPU acceleration
4. âœ… Efficient memory usage and low latency
5. âœ… Comprehensive performance monitoring

This approach provides the best of both worlds: NPU acceleration when beneficial, CPU efficiency when appropriate, all transparent to the user.