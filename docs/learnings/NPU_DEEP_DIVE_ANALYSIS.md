# NPU Compatibility Deep Dive Analysis

## üîç Key Discovery: Why Embeddings Work vs Chat Models

### ‚úÖ Embedding Model Success Pattern
```python
# From embedding_engine.py - WORKING NPU approach:
session_npu = ort.InferenceSession(
    model_file,  # Single model.onnx file
    providers=['QNNExecutionProvider', 'CPUExecutionProvider']
)

# Characteristics that make it NPU-compatible:
- Single ONNX file (~50-200MB typical BERT model)
- Fixed input/output shapes
- Standard transformer operations (attention, layer norm)
- No autoregressive generation
- Batch processing friendly
```

### ‚ùå Chat Model Failure Pattern
```python
# Current approach - FAILING on NPU:
decoder_model_merged_q4.onnx  # 1.4GB complex decoder
embed_tokens_quantized.onnx   # 2.7GB embedding matrices

# Issues causing NPU failure:
- Multi-file architecture with large sizes
- Autoregressive generation (dynamic shapes)
- Complex quantization (q4) not fully supported
- Graph complexity exceeding QNN limits
```

## üöÄ Solution Strategy: NPU-Compatible Llama Architecture

### Meta's NPU Optimization Approach
Based on Qualcomm documentation and Meta's collaboration:

1. **Unified ONNX Models**: Single-file architecture like embeddings
2. **Smaller Quantization**: INT8 instead of INT4 for better QNN support
3. **Static Shapes**: Pre-compiled for specific sequence lengths
4. **Simplified Operations**: Reduced to QNN-supported operations

### Recommended Models to Try

#### Option 1: Microsoft Olive Optimized Models
```
microsoft/llama-3.2-3b-instruct-olive-onnx
- Specifically optimized for ONNX Runtime
- NPU-friendly quantization
- Single-file architecture
```

#### Option 2: Qualcomm Official Examples
```
qualcomm-ai/llama-3.2-3b-qnn
- Official QNN-optimized version
- Guaranteed compatibility with QNNExecutionProvider
```

#### Option 3: ONNX Model Zoo
```
onnx/models/llama-3.2-3b-instruct-dynamic
- Community-maintained
- Multiple quantization options
```

## üõ†Ô∏è Implementation Strategy

### Phase 1: Model Architecture Refactor
Modify our ChatModel to use embedding-style single-file loading:

```python
class OptimizedChatModel(BaseModel):
    """NPU-optimized chat model following embedding pattern"""
    
    def __init__(self):
        self.model_file = "llama-3.2-3b-unified.onnx"  # Single file
        self.session_npu = None
        self.session_cpu = None
        
    def _initialize_sessions(self):
        # Copy exact pattern from successful embedding model
        self.session_npu = ort.InferenceSession(
            self.model_file,
            providers=['QNNExecutionProvider', 'CPUExecutionProvider']
        )
```

### Phase 2: Download NPU-Optimized Model
1. Try Microsoft Olive-optimized Llama
2. Fall back to community ONNX models
3. Convert existing model using ONNX simplifier

### Phase 3: Validate NPU Usage
```python
# Verify NPU is actually being used
actual_providers = session.get_providers()
if 'QNNExecutionProvider' in actual_providers:
    logger.info("‚úÖ NPU is active!")
```

## üéØ Next Actions

1. **Download unified Llama 3.2 3B ONNX model**
2. **Refactor ChatModel to match EmbeddingEngine pattern**
3. **Test NPU compatibility**
4. **Compare performance: NPU vs CPU**

## üìä Expected Results

If we follow the embedding model pattern:
- ‚úÖ NPU should work (same architecture that works for embeddings)
- üöÄ 2-3x faster inference on small batches
- üíæ Lower memory usage
- ‚ö° Better power efficiency

## üîÑ Fallback Plan

If NPU still fails:
- Keep CPU implementation
- Focus on optimizing CPU performance
- Wait for QNN provider updates
- Document learnings for future models